import os
from transformers import pipeline
from datasets import load_dataset, Audio
import evaluate
from tqdm import tqdm
import argparse

REF_KEY = "sentence"
HF_API_KEY = "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"

def transcribe(whisper_asr, audio):
    """
    Transcribe the given audio using the Whisper ASR model.

    Args:
        whisper_asr (transformers.Pipeline): The Whisper ASR Hugging Face pipeline
        audio (dict): The audio data to transcribe

    Returns:
        text (str): The  model output text from the audio
    """

    text = whisper_asr(audio)["text"]
    return text

def evaluation(dataset, whisper_asr):
    """
    Evaluate the model on a dataset

    Args:
        dataset (datasets.Dataset): The dataset containing audio samples and transcriptions.
        whisper_asr (transformers.Pipeline): The Whisper ASR Hugging Face pipeline.

    Returns:
        predictions (list of str): List of transcriptions generated by the model.
        eferences (list of str): List of reference transcriptions from the dataset.
    """

    predictions, references = [], []

    # loop through each sample and get the output and ref
    for item in tqdm(dataset, desc='Evaluating Progress'):
        audio_data = item["audio"]
        text = transcribe(whisper_asr, audio_data)
        predictions.append(text)
        references.append(item[REF_KEY])
    
    return predictions, references

def main(args):
    print(f"[INFO] Evaluting {args.model_name} with {args.dataset_name} with language {args.language}/{args.config}")

    wer_metric = evaluate.load("wer")


    # load model
    whisper_asr = pipeline(
        "automatic-speech-recognition", model=args.model_name, device=0
    )

    whisper_asr.model.config.forced_decoder_ids = (
        whisper_asr.tokenizer.get_decoder_prompt_ids(
            language=args.language, task="transcribe"
        )
    )

    # load the dataset and downsample audio data to 16kHz
    dataset = load_dataset(args.dataset_name, args.config, split="test", token=HF_API_KEY)
    dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))

    # evaluate
    predictions, references = evaluation(dataset, whisper_asr)
    wer = wer_metric.compute(references=references, predictions=predictions)

    # output results to a file
    os.makedirs(args.output_dir, exist_ok=True)
    with open(os.path.join(args.output_dir, f'model_results_{args.language}.txt'), 'w') as file:
        file.write(f"[INFO] Evaluated {args.model_name} with {args.dataset_name} with language {args.language}/{args.config}\n")
        file.write(f"WER : {round(100 * wer, 2)}\n\n")
        if args.save_transcript:
            for ref, pred in zip(references, predictions):
                file.write(f"Reference: {ref}\nPrediction: {pred}\n{'-' * 40}\n")

    print(f"[INFO] Results saved to {args.output_dir}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Whisper ASR Evaluation")
    parser.add_argument("--model_name", required=False, type=str, default="openai/whisper-small", help="Hugging Face Model name, or directory to the model")
    parser.add_argument("--dataset_name", required=False, type=str, default="mozilla-foundation/common_voice_13_0", help="Name of the dataset from Hugging Face")
    parser.add_argument("--config", required=False, type=str, default="vi_vn", help="Dataset configuration. Ex 'vi_vn' for Google Fleurs")
    parser.add_argument("--language", required=False, type=str, default="vi", help="Language for transcription")
    parser.add_argument("--device", required=False, type=int, default=0, help="GPU ID (ex: 0), to use CPU use (-1)")
    parser.add_argument("--split", required=False, type=str, default="test", help="The split to evaluate on. (ex: 'test')")
    parser.add_argument("--output_dir", required=False, type=str, default="whisper_eval", help="Directory to save the evaluation results")
    parser.add_argument("--save_transcript", action='store_true', help="Flag to save transcript to a file")
    args = parser.parse_args()

    main(args)